{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca13fd4a",
   "metadata": {},
   "source": [
    "OG Note: This demo is adapted from the LXMERT Demo present here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/lxmert\n",
    "\n",
    "JN Note: The bulk of the code was sourced from: https://github.com/huggingface/transformers/tree/main/examples/research_projects/visual_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b87589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Local packages\n",
    "from processing_image import Preprocess\n",
    "from visualizing_image import SingleImageViz\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n",
    "\n",
    "# Model\n",
    "from transformers import LxmertForQuestionAnswering, LxmertTokenizer\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "output_title_base='/u/scr/nlp/data/nli-consistency/lxmert_results/lxmert-test-3pred-40token-1seed_predictions'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "# Data from https://visualgenome.org/api/v0/api_home.html\n",
    "# Used Version 1.2 as that one had Part 1 and Part 2 (selected Part 1)\n",
    "VG_path = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K/'\n",
    "VG_path2 = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K_2/'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "# L-ConVQA from https://arijitray1993.github.io/ConVQA/\n",
    "file_source='/u/scr/nlp/data/nli-consistency/cleaned_Logical_ConVQA_test.json'\n",
    "file_paths=[]\n",
    "with open(file_source, 'r') as stream:\n",
    "    data = json.load(stream)\n",
    "    \n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "# VQA_URL = \"https://raw.githubusercontent.com/airsplay/lxmert/master/data/vqa/trainval_label2ans.json\"\n",
    "VQA_URL = '/u/scr/nlp/data/nli-consistency/trainval_label2ans.json' # downloaded version from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c0887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answer labels\n",
    "vqa_answers = utils.get_data(VQA_URL)\n",
    "\n",
    "def convert_multiple_answers(predictions, vqa_answers):\n",
    "    return [vqa_answers[predictions[i]] for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82356cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /sailhome/jnoh2/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "frcnn_cfg.model.device = device\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert_vqa = LxmertForQuestionAnswering.from_pretrained(\"unc-nlp/lxmert-vqa-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0741deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/725 [00:00<?, ?it/s]/nlp/scr/jnoh2/miniconda3/envs/nli-consistency-vilt/lib/python3.9/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "/nlp/scr/jnoh2/miniconda3/envs/nli-consistency-vilt/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755849709/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 725/725 [04:43<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Partially based on code from :\n",
    "# https://huggingface.co/docs/transformers/model_doc/visual_bert\n",
    "\n",
    "predictions = {}\n",
    "num_predictions = 3\n",
    "\n",
    "# for key in cleaned_data.keys():\n",
    "#     image_num = key\n",
    "#     image_path = VG_path + str(image_num) + '.jpg'\n",
    "    \n",
    "#     qas = cleaned_data[key]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "no_image = []\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "# It takes 4.1 seconds for 20 images; 5 images per second\n",
    "# for key in data.keys():\n",
    "data_keys = list(data.keys())\n",
    "for i in tqdm(range(len(data_keys))):\n",
    "    key = data_keys[i]\n",
    "    image_num = key\n",
    "    \n",
    "    image_path1 = VG_path + str(image_num) + '.jpg'\n",
    "    image_path2 = VG_path2 + str(image_num) + '.jpg'\n",
    "    \n",
    "    if exists(image_path1):\n",
    "        image_path = image_path1\n",
    "    elif exists(image_path2):\n",
    "        image_path = image_path2\n",
    "    else:\n",
    "        no_image.append(image_num)\n",
    "        continue\n",
    "    \n",
    "    # From the main demo\n",
    "    images, sizes, scales_yx = image_preprocess(image_path)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    normalized_boxes = output_dict.get(\"normalized_boxes\")\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    # From the main demo\n",
    "    \n",
    "    predictions[image_num] = {}\n",
    "    \n",
    "    qas = data[key]\n",
    "    \n",
    "    # Sets of consistent questions\n",
    "    for j in range(len(qas)):\n",
    "        qs_only = [qas[j][k]['question'] for k in range(len(qas[j]))]\n",
    "        inputs = lxmert_tokenizer(\n",
    "            qs_only, \n",
    "            return_tensors='pt', \n",
    "            padding = \"max_length\", \n",
    "            max_length=40,\n",
    "            truncation = True, \n",
    "            add_special_tokens = True, \n",
    "            return_token_type_ids=True, \n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        output_vqa = lxmert_vqa(\n",
    "            input_ids=inputs.input_ids.to(device),\n",
    "            attention_mask=inputs.attention_mask.to(device),\n",
    "            visual_feats=features.expand(len(inputs.input_ids), -1, -1).to(device),\n",
    "            visual_pos=normalized_boxes.expand(len(inputs.input_ids), -1, -1).to(device),\n",
    "            token_type_ids=inputs.token_type_ids.to(device),\n",
    "            output_attentions=False,\n",
    "        )\n",
    "        \n",
    "        final_logits = output_vqa['question_answering_score']\n",
    "        final_softmax = final_logits.softmax(dim = -1)\n",
    "        final_sorted, final_indices = torch.sort(final_softmax, dim = -1, descending = True)\n",
    "        \n",
    "        predictions[image_num][j] = [qas[j][k]|{'prediction':convert_multiple_answers(final_indices[k][0:num_predictions], vqa_answers), 'prob':final_sorted[k][0:num_predictions].tolist()} for k in range(len(qas[j]))]\n",
    "        \n",
    "    counter += 1    \n",
    "    \n",
    "total_time = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f631d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6559028292104874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "qa_correct = []\n",
    "qa_incorrect = []\n",
    "\n",
    "for key in predictions.keys():\n",
    "    for set_num in predictions[key].keys():\n",
    "        for qas in predictions[key][set_num]:\n",
    "            if qas['answer'] != qas['prediction'][0]:\n",
    "                incorrect += 1\n",
    "                qa_incorrect.append((qas['answer'], qas['prediction'][0]))\n",
    "            else:\n",
    "                correct += 1\n",
    "                qa_correct.append((qas['answer'], qas['prediction'][0]))\n",
    "\n",
    "accuracy = float(correct) / (float(correct) + float(incorrect))\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecfeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_title_base + '.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "with open(output_title_base + '.txt', 'w') as f:\n",
    "    json.dump({'accuracy: ':accuracy, \n",
    "               'qa_correct: ':len(qa_correct), \n",
    "               'qa_incorrect: ':len(qa_incorrect), \n",
    "               'no_image: ':len(no_image),\n",
    "               'total_questions: ':len(qa_correct) + len(qa_incorrect) + len(no_image),\n",
    "               'time_taken: ':total_time,\n",
    "              }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0298bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6559028292104874"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8807fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.385799099"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7006e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
