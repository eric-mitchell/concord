{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca13fd4a",
   "metadata": {},
   "source": [
    "OG Note: This demo is adapted from the LXMERT Demo present here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/lxmert\n",
    "\n",
    "JN Note: The bulk of the code was sourced from: https://github.com/huggingface/transformers/tree/main/examples/research_projects/visual_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b87589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Local packages\n",
    "from processing_image import Preprocess\n",
    "from visualizing_image import SingleImageViz\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n",
    "\n",
    "# Model\n",
    "from transformers import LxmertForQuestionAnswering, LxmertTokenizer\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH FOR PREDICTION\n",
    "output_title_base='/u/scr/nlp/data/nli-consistency/vqa-camera/lxmert_results/lxmert-run-train-10000im-3pred-40token-1seed_predictions'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH FOR PREDICTION\n",
    "# Data from https://visualgenome.org/api/v0/api_home.html\n",
    "# Used Version 1.2 as that one had Part 1 and Part 2 (selected Part 1)\n",
    "VG_path = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K/'\n",
    "VG_path2 = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K_2/'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH FOR PREDICTION\n",
    "# L-ConVQA from https://arijitray1993.github.io/ConVQA/\n",
    "file_source='/u/scr/nlp/data/nli-consistency/vg-data-2022-06-13-16:03:37-n=10000-seed=1.txt'\n",
    "file_paths=[]\n",
    "with open(file_source, 'r') as stream:\n",
    "    for line in stream:\n",
    "        file_paths.append(line[:-1])\n",
    "file_names = [re.search(r'qas_(.*?)\\.json', path).group(1) for path in file_paths] # Gets the image number\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH FOR PREDICTION\n",
    "# Test set\n",
    "cleaned_path = '/u/scr/nlp/data/nli-consistency/cleaned_Logical_ConVQA_test.json'\n",
    "with open(cleaned_path, 'r') as r_file:\n",
    "    cleaned_data = json.load(r_file)\n",
    "    \n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH FOR PREDICTION\n",
    "# VQA_URL = \"https://raw.githubusercontent.com/airsplay/lxmert/master/data/vqa/trainval_label2ans.json\"\n",
    "VQA_URL = '/u/scr/nlp/data/nli-consistency/trainval_label2ans.json' # downloaded version from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load answer labels\n",
    "vqa_answers = utils.get_data(VQA_URL)\n",
    "\n",
    "def convert_multiple_answers(predictions, vqa_answers):\n",
    "    return [vqa_answers[predictions[i]] for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82356cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "frcnn_cfg.model.device = device\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "lxmert_tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "lxmert_vqa = LxmertForQuestionAnswering.from_pretrained(\"unc-nlp/lxmert-vqa-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7976a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partially based on code from :\n",
    "# https://huggingface.co/docs/transformers/model_doc/visual_bert\n",
    "\n",
    "predictions = {}\n",
    "num_predictions = 3\n",
    "\n",
    "# for key in cleaned_data.keys():\n",
    "#     image_num = key\n",
    "#     image_path = VG_path + str(image_num) + '.jpg'\n",
    "    \n",
    "#     qas = cleaned_data[key]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "no_image = []\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "# It takes 4.1 seconds for 20 images; 5 images per second\n",
    "for i in tqdm(range(len(file_paths))):\n",
    "# for i in tqdm(range(1000)):\n",
    "    file_path = file_paths[i]\n",
    "    image_num = file_names[i]\n",
    "    \n",
    "    image_path1 = VG_path + str(image_num) + '.jpg'\n",
    "    image_path2 = VG_path2 + str(image_num) + '.jpg'\n",
    "    \n",
    "    if exists(image_path1):\n",
    "        image_path = image_path1\n",
    "    elif exists(image_path2):\n",
    "        image_path = image_path2\n",
    "    else:\n",
    "        no_image.append(image_num)\n",
    "        continue\n",
    "    \n",
    "    # From the main demo\n",
    "    images, sizes, scales_yx = image_preprocess(image_path)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    normalized_boxes = output_dict.get(\"normalized_boxes\")\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    # From the main demo\n",
    "    \n",
    "    predictions[image_num] = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file_stream:\n",
    "        qas = json.load(file_stream)['consistent']\n",
    "    \n",
    "    # Sets of consistent questions\n",
    "    for j in range(len(qas)):\n",
    "        qs_only = [qas[j][k]['question'] for k in range(len(qas[j]))]\n",
    "        inputs = lxmert_tokenizer(\n",
    "            qs_only, \n",
    "            return_tensors='pt', \n",
    "            padding = \"max_length\", \n",
    "            max_length=40,\n",
    "            truncation = True, \n",
    "            add_special_tokens = True, \n",
    "            return_token_type_ids=True, \n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        output_vqa = lxmert_vqa(\n",
    "            input_ids=inputs.input_ids.to(device),\n",
    "            attention_mask=inputs.attention_mask.to(device),\n",
    "            visual_feats=features.expand(len(inputs.input_ids), -1, -1).to(device),\n",
    "            visual_pos=normalized_boxes.expand(len(inputs.input_ids), -1, -1).to(device),\n",
    "            token_type_ids=inputs.token_type_ids.to(device),\n",
    "            output_attentions=False,\n",
    "        )\n",
    "        \n",
    "        final_logits = output_vqa['question_answering_score']\n",
    "        final_softmax = final_logits.softmax(dim = -1)\n",
    "        final_sorted, final_indices = torch.sort(final_softmax, dim = -1, descending = True)\n",
    "        \n",
    "        predictions[image_num][j] = [qas[j][k]|{'prediction':convert_multiple_answers(final_indices[k][0:num_predictions], vqa_answers), 'prob':final_sorted[k][0:num_predictions].tolist()} for k in range(len(qas[j]))]\n",
    "        \n",
    "    counter += 1    \n",
    "    \n",
    "total_time = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f631d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "qa_correct = []\n",
    "qa_incorrect = []\n",
    "\n",
    "for key in predictions.keys():\n",
    "    for set_num in predictions[key].keys():\n",
    "        for qas in predictions[key][set_num]:\n",
    "            if qas['answer'] != qas['prediction'][0]:\n",
    "                incorrect += 1\n",
    "                qa_incorrect.append((qas['answer'], qas['prediction'][0]))\n",
    "            else:\n",
    "                correct += 1\n",
    "                qa_correct.append((qas['answer'], qas['prediction'][0]))\n",
    "\n",
    "accuracy = float(correct) / (float(correct) + float(incorrect))\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_title_base + '.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "with open(output_title_base + '.txt', 'w') as f:\n",
    "    json.dump({'accuracy: ':accuracy, \n",
    "               'qa_correct: ':len(qa_correct), \n",
    "               'qa_incorrect: ':len(qa_incorrect), \n",
    "               'no_image: ':len(no_image),\n",
    "               'total_questions: ':len(qa_correct) + len(qa_incorrect) + len(no_image),\n",
    "               'time_taken: ':total_time,\n",
    "              }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8807fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7006e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
