{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca13fd4a",
   "metadata": {},
   "source": [
    "OG Note: This demo is adapted from the LXMERT Demo present here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/lxmert\n",
    "\n",
    "JN Note: The bulk of the code was sourced from: https://github.com/huggingface/transformers/tree/main/examples/research_projects/visual_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b87589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Model\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "output_title_base='/u/scr/nlp/data/nli-consistency/vilt_results/vilt-test-3pred-40token-1seed_predictions'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "# Data from https://visualgenome.org/api/v0/api_home.html\n",
    "# Used Version 1.2 as that one had Part 1 and Part 2 (selected Part 1)\n",
    "VG_path = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K/'\n",
    "VG_path2 = '/u/scr/nlp/data/nli-consistency/vg_data/VG_100K_2/'\n",
    "\n",
    "### INSTRUCTION FOR USERS : INDICATE APPROPRIATE PATH\n",
    "# L-ConVQA from https://arijitray1993.github.io/ConVQA/\n",
    "file_source='/u/scr/nlp/data/nli-consistency/cleaned_Logical_ConVQA_test.json'\n",
    "file_paths=[]\n",
    "with open(file_source, 'r') as stream:\n",
    "    data = json.load(stream)\n",
    "    \n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c0887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multiple_answers(predictions, model):\n",
    "    return [model.config.id2label[predictions[i].item()] for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51677d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vilt\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0741deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 725/725 [04:09<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Partially based on code from :\n",
    "# https://huggingface.co/docs/transformers/model_doc/visual_bert\n",
    "\n",
    "predictions = {}\n",
    "num_predictions = 3\n",
    "\n",
    "# for key in cleaned_data.keys():\n",
    "#     image_num = key\n",
    "#     image_path = VG_path + str(image_num) + '.jpg'\n",
    "    \n",
    "#     qas = cleaned_data[key]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "no_image = []\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "# It takes 4.1 seconds for 20 images; 5 images per second\n",
    "data_keys = list(data.keys())\n",
    "for i in tqdm(range(len(data_keys))):\n",
    "    key = data_keys[i]\n",
    "    image_num = key\n",
    "    \n",
    "    image_path1 = VG_path + str(image_num) + '.jpg'\n",
    "    image_path2 = VG_path2 + str(image_num) + '.jpg'\n",
    "    \n",
    "    if exists(image_path1):\n",
    "        image_path = image_path1\n",
    "    elif exists(image_path2):\n",
    "        image_path = image_path2\n",
    "    else:\n",
    "        no_image.append(image_num)\n",
    "        continue\n",
    "    \n",
    "    # From the main demo\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    predictions[image_num] = {}\n",
    "    \n",
    "    qas = data[key]\n",
    "    \n",
    "    # Sets of consistent questions\n",
    "    for j in range(len(qas)):\n",
    "        predictions[image_num][j] = []\n",
    "        for k in range(len(qas[j])):\n",
    "            qs_only = qas[j][k]['question']\n",
    "            image\n",
    "            encoding = processor(\n",
    "                image,\n",
    "                qs_only,\n",
    "                return_tensors='pt', \n",
    "                padding = \"max_length\", \n",
    "                max_length=40,\n",
    "                truncation = True, \n",
    "                add_special_tokens = True, \n",
    "                return_token_type_ids=True, \n",
    "                return_attention_mask=True,\n",
    "            ).to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            l_softmax = logits.softmax(dim = -1)\n",
    "            l_sorted, l_indices = torch.sort(l_softmax, dim=-1, descending=True)\n",
    "            \n",
    "            predictions[image_num][j].append(qas[j][k]|{'prediction':convert_multiple_answers(l_indices[0][0:num_predictions], model), 'prob':l_sorted[0][0:num_predictions].tolist()})\n",
    "        \n",
    "    counter += 1    \n",
    "    \n",
    "total_time = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f631d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7840319952599615"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "qa_correct = []\n",
    "qa_incorrect = []\n",
    "\n",
    "for key in predictions.keys():\n",
    "    for set_num in predictions[key].keys():\n",
    "        for qas in predictions[key][set_num]:\n",
    "            if qas['answer'] != qas['prediction'][0]:\n",
    "                incorrect += 1\n",
    "                qa_incorrect.append((qas['answer'], qas['prediction'][0]))\n",
    "            else:\n",
    "                correct += 1\n",
    "                qa_correct.append((qas['answer'], qas['prediction'][0]))\n",
    "\n",
    "accuracy = float(correct) / (float(correct) + float(incorrect))\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecfeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_title_base + '.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "with open(output_title_base + '.txt', 'w') as f:\n",
    "    json.dump({'accuracy: ':accuracy, \n",
    "               'qa_correct: ':len(qa_correct), \n",
    "               'qa_incorrect: ':len(qa_incorrect), \n",
    "               'no_image: ':len(no_image),\n",
    "               'total_questions: ':len(qa_correct) + len(qa_incorrect) + len(no_image),\n",
    "               'time_taken: ':total_time,\n",
    "              }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0298bed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7840319952599615"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8807fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495.394285222"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7006e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406c172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
